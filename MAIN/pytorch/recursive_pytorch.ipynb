{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Markov Switching Model using PyTorch\n",
    "class MarkovSwitchingModel(nn.Module):\n",
    "    def __init__(self, num_states=2):\n",
    "        super(MarkovSwitchingModel, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        # Unconstrained means\n",
    "        self.mu = nn.Parameter(torch.zeros(num_states, device=device))\n",
    "        # Unconstrained log variances\n",
    "        self.log_sigma2 = nn.Parameter(torch.zeros(num_states, device=device))\n",
    "        # Unconstrained transition logits\n",
    "        self.transition_logits = nn.Parameter(torch.zeros(num_states, num_states, device=device))\n",
    "        # Initial state logits\n",
    "        self.initial_logits = nn.Parameter(torch.zeros(num_states, device=device))\n",
    "    \n",
    "    def forward(self, y):\n",
    "        T = y.shape[0]\n",
    "        mu = self.mu  # Shape: [num_states]\n",
    "        sigma2 = torch.exp(self.log_sigma2)  # Ensure variances are positive\n",
    "        \n",
    "        # Compute log emission probabilities\n",
    "        emission_log_probs = -0.5 * (torch.log(2 * torch.pi * sigma2[:, None]) + ((y - mu[:, None]) ** 2) / sigma2[:, None])\n",
    "        \n",
    "        # Compute log transition probabilities\n",
    "        log_transition_probs = F.log_softmax(self.transition_logits, dim=1)  # Shape: [num_states, num_states]\n",
    "        # Initial state log probabilities\n",
    "        log_initial_probs = F.log_softmax(self.initial_logits, dim=0)  # Shape: [num_states]\n",
    "        \n",
    "        # Forward algorithm in log-space\n",
    "        log_alpha = log_initial_probs + emission_log_probs[:, 0]  # Shape: [num_states]\n",
    "        for t in range(1, T):\n",
    "            log_alpha = emission_log_probs[:, t] + torch.logsumexp(log_alpha[:, None] + log_transition_probs, dim=0)\n",
    "        # Compute log-likelihood\n",
    "        log_likelihood = torch.logsumexp(log_alpha, dim=0)\n",
    "        return -log_likelihood  # Negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute smoothed state probabilities\n",
    "def compute_forward_backward(model, y):\n",
    "    T = y.shape[0]\n",
    "    num_states = model.num_states\n",
    "    mu = model.mu\n",
    "    sigma2 = torch.exp(model.log_sigma2)\n",
    "    \n",
    "    emission_log_probs = -0.5 * (torch.log(2 * torch.pi * sigma2[:, None]) + ((y - mu[:, None]) ** 2) / sigma2[:, None])\n",
    "    log_transition_probs = F.log_softmax(model.transition_logits, dim=1)\n",
    "    log_initial_probs = F.log_softmax(model.initial_logits, dim=0)\n",
    "    \n",
    "    # Forward pass\n",
    "    log_alpha = log_initial_probs + emission_log_probs[:, 0]\n",
    "    log_alpha_list = [log_alpha]\n",
    "    for t in range(1, T):\n",
    "        log_alpha = emission_log_probs[:, t] + torch.logsumexp(log_alpha[:, None] + log_transition_probs, dim=0)\n",
    "        log_alpha_list.append(log_alpha)\n",
    "    log_alpha_tensor = torch.stack(log_alpha_list, dim=1)\n",
    "    \n",
    "    # Backward pass\n",
    "    log_beta = torch.zeros(num_states, device=device)\n",
    "    log_beta_list = [log_beta]\n",
    "    for t in reversed(range(T - 1)):\n",
    "        log_beta = torch.logsumexp(log_transition_probs + emission_log_probs[:, t + 1] + log_beta[:, None], dim=0)\n",
    "        log_beta_list.insert(0, log_beta)\n",
    "    log_beta_tensor = torch.stack(log_beta_list, dim=1)\n",
    "    \n",
    "    # Compute smoothed probabilities\n",
    "    log_gamma = log_alpha_tensor + log_beta_tensor\n",
    "    log_gamma -= torch.logsumexp(log_gamma, dim=0, keepdim=True)\n",
    "    gamma = torch.exp(log_gamma)\n",
    "    return gamma\n",
    "\n",
    "# Function to fit the model to the data\n",
    "def fit_model(y, num_states=2, num_epochs=100, lr=0.01):\n",
    "    model = MarkovSwitchingModel(num_states=num_states).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        nll = model(y_tensor)\n",
    "        nll.backward()\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each date\n",
    "def process_date(args):\n",
    "    i, date = args\n",
    "    # Expand the training data to include up to the current date\n",
    "    recursive_train = data.loc[data.index <= date, 'Index_Returns'].values\n",
    "    y = torch.tensor(recursive_train, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Fit the Markov Switching Model on the expanded training data\n",
    "    try:\n",
    "        model = fit_model(y, num_states=2, num_epochs=100, lr=0.01)\n",
    "    except Exception as e:\n",
    "        print(f\"Model failed to converge at date {date}: {e}\")\n",
    "        return None  # Skip this date if the model fails to fit\n",
    "    \n",
    "    # Compute smoothed probabilities\n",
    "    gamma = compute_forward_backward(model, y)  # Shape: [num_states, T]\n",
    "    # Get last known state probabilities\n",
    "    last_probs = gamma[:, -1].cpu().detach().numpy()\n",
    "    \n",
    "    # Extract transition probabilities from the model parameters\n",
    "    with torch.no_grad():\n",
    "        transition_probs = F.softmax(model.transition_logits, dim=1).cpu().numpy()\n",
    "        sigma2 = torch.exp(model.log_sigma2).cpu().numpy()\n",
    "    \n",
    "    # Update state probabilities to predict the next day's regime\n",
    "    state_probs = last_probs @ transition_probs  # Shape: [num_states]\n",
    "    \n",
    "    # Determine the most likely regime at t+1\n",
    "    regime_labels = list(range(model.num_states))  # Should be [0, 1]\n",
    "    most_likely_regime = regime_labels[np.argmax(state_probs)]\n",
    "    \n",
    "    # Get the next date for prediction\n",
    "    if i + 1 < len(test.index):\n",
    "        next_date = test.index[i + 1]\n",
    "    else:\n",
    "        # Predicting beyond the available data; estimate next date\n",
    "        next_date = date + pd.Timedelta(days=1)\n",
    "        # Add next_date to the DataFrame if it doesn't exist\n",
    "        if next_date not in data.index:\n",
    "            data.loc[next_date] = np.nan  # Initialize with NaNs\n",
    "    \n",
    "    # Determine which regime corresponds to low and high volatility\n",
    "    if sigma2[0] < sigma2[1]:\n",
    "        regime_mapping = {0: 'Low Volatility', 1: 'High Volatility'}\n",
    "    else:\n",
    "        regime_mapping = {0: 'High Volatility', 1: 'Low Volatility'}\n",
    "    \n",
    "    # Map the predicted regime to labels\n",
    "    predicted_label = regime_mapping[most_likely_regime]\n",
    "    \n",
    "    # Return the results\n",
    "    return next_date, most_likely_regime, predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the arguments for processing\n",
    "test_dates = test.index\n",
    "args_list = [(i, date) for i, date in enumerate(test_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dates and collect results\n",
    "# ****THIS TAKES FOREVER TO RUN ***\n",
    "results = []\n",
    "for args in args_list:\n",
    "    res = process_date(args)\n",
    "    if res is not None:\n",
    "        results.append(res)\n",
    "\n",
    "# Update the data DataFrame with the results\n",
    "for next_date, most_likely_regime, predicted_label in results:\n",
    "    data.at[next_date, 'Recursive_Predictions'] = most_likely_regime\n",
    "    data.at[next_date, 'Recursive_Predicted_Regime_Label'] = predicted_label\n",
    "\n",
    "# Display the recursive predictions\n",
    "print(data.loc[test.index, ['Index_Returns', 'Recursive_Predictions', 'Recursive_Predicted_Regime_Label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_test)",
   "language": "python",
   "name": "cuda_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
